{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b0ebaf",
   "metadata": {},
   "source": [
    "Q1: Edge AI vs Cloud AI — Latency and Privacy\n",
    "Edge AI performs AI computations directly on local devices (eg, drones, smartphones, sensors), whereas Cloud AI sends data to remote servers for processing. This difference leads to two major advantages for Edge AI:\n",
    "\n",
    "1. Reduced Latency: Since data doesn't need to travel to the cloud and back, decisions can be made in real time. This is critical for time-sensitive applications like autonomous drones, where milliseconds matter for obstacle avoidance or navigation.\n",
    "2. Enhanced Privacy: Sensitive data (e.g., video feeds, biometric inputs) stays on the device, minimizing exposure to breaches or unauthorized access during transmission or storage in the cloud.\n",
    "Real-world example — Autonomous Drones:\n",
    "- In search-and-rescue missions, drones equipped with Edge AI can analyze terrain, detect humans, and make navigation decisions instantly without relying on cloud connectivity.\n",
    "- This ensures faster response times and secure handling of sensitive imagery, especially in remote or disaster-struck areas where connectivity is unreliable.\n",
    "\n",
    " Q2: Quantum AI vs Classical AI — Optimization Power\n",
    "Classical AI uses traditional computing methods (binary bits) and algorithms like gradient descent or heuristics to solve optimization problems. These methods work well but struggle with combinatorial complexity and local minima traps in large-scale problems.\n",
    "Quantum AI, powered by quantum computing, leverages qubits and principles like superposition and entanglement to explore multiple solutions simultaneously. This enables:\n",
    "- Exponential speedups in solving problems like the traveling salesman, portfolio optimization, or molecular simulations.\n",
    "- Better global optima discovery in complex landscapes.\n",
    "Industries poised to benefit most:\n",
    "- Logistics: Route optimization, warehouse management, and supply chain coordination.\n",
    "- Finance: Risk modeling, fraud detection, and portfolio optimization.\n",
    "- Pharmaceuticals: Drug discovery through molecular modeling and protein folding.\n",
    "- Energy: Grid optimization and predictive maintenance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a99cc1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Defined Image Height: 180, Image Width: 180\n",
      "Defined Batch Size: 32\n",
      "Number of classes: 5\n",
      "Class names: ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n",
      "Number of training samples: 93952\n",
      "Number of validation samples: 23488\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# Define image size and batch size\n",
    "IMG_HEIGHT = 180\n",
    "IMG_WIDTH = 180\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Defined Image Height: {IMG_HEIGHT}, Image Width: {IMG_WIDTH}\")\n",
    "print(f\"Defined Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# Download and load the tf_flowers dataset\n",
    "# The 'as_supervised=True' argument loads the dataset as (image, label) pairs.\n",
    "# The 'with_info=True' argument returns a tfds.core.DatasetInfo object.\n",
    "(train_ds_raw, val_ds_raw), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'], # Split the 'train' portion into 80% train and 20% validation\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "class_names = info.features['label'].names\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Number of training samples: {tf.data.experimental.cardinality(train_ds_raw).numpy() * BATCH_SIZE}\")\n",
    "print(f\"Number of validation samples: {tf.data.experimental.cardinality(val_ds_raw).numpy() * BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1adc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed training dataset has 92 batches, each with 32 samples (except possibly the last one).\n",
      "Processed validation dataset has 23 batches, each with 32 samples (except possibly the last one).\n",
      "Approximate number of training samples: 2936\n",
      "Approximate number of validation samples: 734\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Preprocessing function to resize and normalize images\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # Normalize to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing and configure datasets for performance\n",
    "train_ds = train_ds_raw.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds_raw.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Configure for performance\n",
    "train_ds = train_ds.cache().shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Verify the number of samples after splitting and before batching for more accurate count\n",
    "# tf.data.experimental.cardinality returns the number of elements in the dataset, not batches\n",
    "num_train_samples = info.splits['train'].num_examples * 0.8 # Approx 80% of total train split\n",
    "num_val_samples = info.splits['train'].num_examples * 0.2 # Approx 20% of total train split\n",
    "\n",
    "print(f\"Processed training dataset has {len(train_ds)} batches, each with {BATCH_SIZE} samples (except possibly the last one).\")\n",
    "print(f\"Processed validation dataset has {len(val_ds)} batches, each with {BATCH_SIZE} samples (except possibly the last one).\")\n",
    "print(f\"Approximate number of training samples: {int(num_train_samples)}\")\n",
    "print(f\"Approximate number of validation samples: {int(num_val_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24640ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_4460\\758348507.py:7: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,405</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m6,405\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,264,389</span> (8.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,264,389\u001b[0m (8.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,405</span> (25.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,405\u001b[0m (25.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 879ms/step - accuracy: 0.6669 - loss: 0.8738 - val_accuracy: 0.8488 - val_loss: 0.4464\n",
      "Epoch 2/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 694ms/step - accuracy: 0.8345 - loss: 0.4800 - val_accuracy: 0.8706 - val_loss: 0.3735\n",
      "Epoch 3/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 691ms/step - accuracy: 0.8542 - loss: 0.4075 - val_accuracy: 0.8815 - val_loss: 0.3464\n",
      "Epoch 4/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 700ms/step - accuracy: 0.8774 - loss: 0.3478 - val_accuracy: 0.8842 - val_loss: 0.3394\n",
      "Epoch 5/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 693ms/step - accuracy: 0.8934 - loss: 0.3093 - val_accuracy: 0.8869 - val_loss: 0.3278\n",
      "Epoch 6/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 700ms/step - accuracy: 0.9046 - loss: 0.2844 - val_accuracy: 0.8869 - val_loss: 0.3291\n",
      "Epoch 7/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 684ms/step - accuracy: 0.9176 - loss: 0.2578 - val_accuracy: 0.8951 - val_loss: 0.3162\n",
      "Epoch 8/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 701ms/step - accuracy: 0.9149 - loss: 0.2476 - val_accuracy: 0.8937 - val_loss: 0.3157\n",
      "Epoch 9/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 683ms/step - accuracy: 0.9275 - loss: 0.2290 - val_accuracy: 0.8856 - val_loss: 0.3188\n",
      "Epoch 10/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 701ms/step - accuracy: 0.9261 - loss: 0.2176 - val_accuracy: 0.8910 - val_loss: 0.3199\n",
      "Model trained for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# 1. Instantiate MobileNetV2 as the base model\n",
    "# include_top=False removes the classifier layer at the top\n",
    "# weights='imagenet' loads weights pre-trained on ImageNet\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze the base model's layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# 2. Create a new classification head\n",
    "# Use Functional API to build the model\n",
    "inputs = tf.keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = base_model(inputs, training=False) # Apply base model in inference mode\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x) # Add a dropout layer for regularization\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# 3. Combine the base model and the new classification head\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "# Print model summary to verify the architecture\n",
    "model.summary()\n",
    "\n",
    "# 4. Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define the number of training epochs\n",
    "EPOCHS = 10\n",
    "\n",
    "# 5. Train the compiled model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds\n",
    ")\n",
    "\n",
    "print(f\"Model trained for {EPOCHS} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed32bf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpi5s5pkrn\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpi5s5pkrn\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpi5s5pkrn'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 180, 180, 3), dtype=tf.float32, name='keras_tensor_154')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1944050033360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050030480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050030288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050028752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050032016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050028944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050031056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050028176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050031248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050033744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050027216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050026832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050027024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050026256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050027600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050034320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050030864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050028560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050033168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050027792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050031632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050025488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050025104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050025872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944050027408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052314960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052316112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052315920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052315536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052315152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052316496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052317456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052317264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052317648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052315344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052316688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052317072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052317840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052316304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944052318032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104075728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104076304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104076112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104075344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104076496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104076880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104077648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104077456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104077840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104075536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104077072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104078608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104078416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104078800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104075920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104078032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104079568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104079376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104079760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104077264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104078992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104080528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104080336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104080720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104078224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104079952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104081488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104081296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104081680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104079184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104080912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104082448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104082256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104082640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104080144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104082832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104083600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104083408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104083792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104081872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104083984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104084752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104084560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104084944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104083024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104084176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104085712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104085520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104085904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104081104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104085136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104086672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104086480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104086864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104084368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104086096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104087632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104087440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104087824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104085328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104087056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104088592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104088400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104088784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104086288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104088016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104089552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104089360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104089744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104087248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104088976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104090512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104090320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104090704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104088208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104089936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104091280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104091088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104090128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104091472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104090896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271782736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271782544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944104089168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271781968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271782160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271783696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271783504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271783888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271782928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271783120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271784656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271784464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271784848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271782352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271784080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271785616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271785424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271785808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271783312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271785040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271786576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271786384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271786768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271784272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271786000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271787536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271787344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271787728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271785232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271786960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271788496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271788304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271788688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271786192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271787920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271789456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271789264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271789648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271787152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271788880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271790416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271790224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271790608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271788112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271789840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271791376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271791184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271791568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271789072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271790800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271792336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271792144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271792528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271790032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271791760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271793296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271793104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271793488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271790992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271792720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271794256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271794064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271794448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271791952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271793680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271795216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271795024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271795408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271792912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271794640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271796176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271795984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271796368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271793872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271795600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271797136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271796944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271797328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271794832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271796560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271797904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271797712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271796752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271798096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271797520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187617744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187617936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944271795792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187617360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187618320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187619280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187619088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187618896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187617552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187618512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187619664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187620048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187620240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187618128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187619472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187620624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187621008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187621392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187618704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187620432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187621584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187621968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187622160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187619856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187621200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187622928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187622352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187623312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187620816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187622544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187623696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187623888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187624080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187621776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187623120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187624848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187624656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187625040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187622736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187624272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187626000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187625424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187625808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187623504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187625232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187626768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187626576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187626960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187624464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187626384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187627536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187627728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187627920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187625616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187628496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1944187629456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TensorFlow Lite model saved as 'flower_classification_model.tflite'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Instantiate a TFLiteConverter object from the trained model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# 2. Enable default optimizations, which include dynamic range quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# 3. Convert the model to TensorFlow Lite format\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 4. Save the converted TensorFlow Lite model to a file\n",
    "with open('flower_classification_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TensorFlow Lite model saved as 'flower_classification_model.tflite'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8633bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model loaded successfully.\n",
      "Input details: [{'name': 'serving_default_keras_tensor_154:0', 'index': 0, 'shape': array([  1, 180, 180,   3], dtype=int32), 'shape_signature': array([ -1, 180, 180,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 172, 'shape': array([1, 5], dtype=int32), 'shape_signature': array([-1,  5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"flower_classification_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"TFLite model loaded successfully.\")\n",
    "print(\"Input details:\", input_details)\n",
    "print(\"Output details:\", output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6244dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model loaded successfully.\n",
      "Input details: [{'name': 'serving_default_keras_tensor_154:0', 'index': 0, 'shape': array([  1, 180, 180,   3], dtype=int32), 'shape_signature': array([ -1, 180, 180,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 172, 'shape': array([1, 5], dtype=int32), 'shape_signature': array([-1,  5], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n",
      "Total samples evaluated: 734\n",
      "Correct predictions: 653\n",
      "TFLite Model Accuracy on Validation Set: 0.8896\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"flower_classification_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"TFLite model loaded successfully.\")\n",
    "print(\"Input details:\", input_details)\n",
    "print(\"Output details:\", output_details)\n",
    "\n",
    "# Prepare to evaluate the TFLite model\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Get input and output tensor indexes\n",
    "input_index = input_details[0]['index']\n",
    "output_index = output_details[0]['index']\n",
    "\n",
    "# Iterate through the validation dataset for inference\n",
    "for images_batch, labels_batch in val_ds:\n",
    "    for i in range(images_batch.shape[0]):\n",
    "        img = images_batch[i]\n",
    "        label = labels_batch[i]\n",
    "\n",
    "        # TFLite models expect input in a specific format, often float32 and batched.\n",
    "        # The image is already preprocessed (resized and normalized to [0,1]) and is float32.\n",
    "        # Add a batch dimension if it's not already there (interpreter expects NCHW or NHWC format).\n",
    "        input_data = np.expand_dims(img, axis=0).astype(input_details[0]['dtype'])\n",
    "\n",
    "        # Set the input tensor\n",
    "        interpreter.set_tensor(input_index, input_data)\n",
    "\n",
    "        # Invoke the interpreter\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output tensor\n",
    "        output_data = interpreter.get_tensor(output_index)\n",
    "\n",
    "        # Get the predicted class (index with highest probability)\n",
    "        predicted_class = np.argmax(output_data)\n",
    "\n",
    "        # Compare with true label\n",
    "        if predicted_class == label.numpy():\n",
    "            correct_predictions += 1\n",
    "        total_samples += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "print(f\"\\nTotal samples evaluated: {total_samples}\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"TFLite Model Accuracy on Validation Set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f18782",
   "metadata": {},
   "source": [
    "Explain Benefits of Edge AI\n",
    " Benefits of Edge AI for Real-time Applications:\n",
    "\n",
    "1.  Reduced Latency: By performing inference directly on the device (e.g., a smartphone, drone, or industrial sensor) rather than sending data to a remote cloud server, Edge AI drastically reduces the time it takes to get a prediction. For real-time applications like autonomous driving, drone navigation (as mentioned in Q1), or real-time object detection in surveillance, milliseconds matter. Our TFLite model, being lightweight and optimized, can run quickly on edge devices, enabling immediate responses.\n",
    "\n",
    "2.  Enhanced Privacy and Security: Sensitive data, such as images from security cameras or personal biometric information, can be processed locally on the device. This minimizes the need to transmit data over networks, reducing the risk of data breaches or unauthorized access during transit or storage in the cloud. For a flower classification model, while less sensitive, this principle scales up significantly for applications dealing with private user data.\n",
    "\n",
    "3.  Lower Bandwidth and Connectivity Costs: Edge AI reduces the amount of data that needs to be sent to the cloud. This saves bandwidth, which can be critical in areas with limited or expensive network access. It also allows applications to function reliably even when offline or in environments with intermittent connectivity, such as remote agricultural fields for crop monitoring or disaster zones for search and rescue.\n",
    "\n",
    "4.  Increased Reliability: Cloud-dependent AI systems can fail if the internet connection is lost or the cloud service experiences downtime. Edge AI systems can operate independently of constant network connectivity, ensuring continuous operation, which is vital for critical real-time systems like medical devices or industrial control systems.\n",
    "\n",
    "5.  Lower Power Consumption on Data Transfer: While running AI models on edge devices consumes power, the overall system can be more energy-efficient by avoiding constant data transmission to and from the cloud, especially for devices powered by batteries.\n",
    "\n",
    "6.  Scalability: Edge AI can enable more devices to participate in an AI ecosystem without overwhelming central cloud servers with massive amounts of raw data. Each edge device handles its own data processing, allowing for distributed and scalable AI deployments.\n",
    "\n",
    "TASK2:AI DRIVEN IOT CONCEPT\n",
    "key sensors needed are;\n",
    "\n",
    "Soil Moisture Sensor: Measures water content in the soil to optimize irrigation.\n",
    "\n",
    "Soil Temperature Sensor: Monitors root-zone temperature, crucial for seed germination and microbial activity.\n",
    "\n",
    "Soil NPK Sensor: Measures the levels of essential macronutrients: Nitrogen (N), Phosphorus (P), and Potassium (K).\n",
    "\n",
    "Soil pH Sensor: Monitors soil acidity/alkalinity, which affects nutrient availability.\n",
    "\n",
    "Leaf Wetness Sensor: Detects moisture on leaf surfaces, helping predict fungal disease outbreaks.\n",
    "\n",
    "Water pH Sensor: (For irrigation systems) Monitors the pH of the water being used.\n",
    "\n",
    "Water Flow Sensor: Measures the amount of water used for irrigation.\n",
    "\n",
    "\n",
    "\n",
    "Core AI Model: Hybrid Ensemble (LSTM + XGBoost)\n",
    "\n",
    "a) Temporal Feature Learning with LSTM (Long Short-Term Memory Network)\n",
    "\n",
    "Purpose: To model time-dependent patterns in the sensor data.\n",
    "\n",
    "Input: Sequential, time-series data from sensors (soil moisture, temperature, humidity, etc.) over the entire growing season.\n",
    "\n",
    "\n",
    "b) Static & Spatial Feature Integration with XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "Purpose: To integrate non-sequential, static, and spatial features.\n",
    "\n",
    "Input:\n",
    "\n",
    "Static Features: Soil type, seed variety, historical field yield data.\n",
    "\n",
    "Spatial Features: Data from multispectral imagery (e.g., NDVI - Normalized Difference Vegetation Index) at different growth stages.\n",
    "\n",
    "Aggregated LSTM Features: The learned features from the LSTM model's hidden layers can be fed into XGBoost as powerful summary inputs.\n",
    "\n",
    "+--------------------------------------------------------------------+\n",
    "|                  AI-DRIVEN SMART AGRICULTURE SYSTEM                |\n",
    "+--------------------------------------------------------------------+\n",
    "\n",
    "+----------------+      +-----------------+      +-----------------+\n",
    "|                |      |                 |      |                 |\n",
    "|  SENSOR LAYER  |----->|   EDGE/GATEWAY  |----->|   CLOUD LAYER   |\n",
    "|                |      |     LAYER       |      |                 |\n",
    "+----------------+      +-----------------+      +-----------------+\n",
    "     |  |  |                    |                        |\n",
    "     |  |  |                    |                        |\n",
    "     v  v  v                    v                        v\n",
    "+-----------+             +-------------+          +-------------+\n",
    "|Soil       | Wireless    |Data         |   Internet  |Data        |\n",
    "|Moisture   |-----------> |Aggregation  |----------> |Storage     |\n",
    "+-----------+   (LoRaWAN) +-------------+            +-------------+\n",
    "+-----------+                               (Data Lake)  |\n",
    "|Temperature|                                            |\n",
    "|& Humidity |                                            |\n",
    "+-----------+                                            |\n",
    "+-----------+                                            |\n",
    "|NPK Sensor |                                            v\n",
    "+-----------+                                    +-------------+\n",
    "+-----------+                                    |AI PROCESSING|\n",
    "|Multispectral|                                   +-------------+\n",
    "|Camera (Drone)|                                  |             |\n",
    "+--------------+                                  | LSTM Model  |\n",
    "                                                  |             |\n",
    "                                                  +-------------+\n",
    "                                                          |\n",
    "                                                          v\n",
    "+----------------+      +-----------------+      +-------------+\n",
    "|                |      |                 |      | PREDICTIONS &|\n",
    "|  ACTION LAYER  |<-----|   DASHBOARD     |<-----|   INSIGHTS   |\n",
    "|                |      |                 |      |             |\n",
    "+----------------+      +-----------------+      +-------------+\n",
    "     ^                         ^                        ^\n",
    "     |                         |                        |\n",
    "+-----------+             +-------------+          +-------------+\n",
    "|Smart      |             |Mobile/Web   |          |Yield        |\n",
    "|Valves     | Control     |Interface    |          |Prediction   |\n",
    "+-----------+  Signals    +-------------+          +-------------+\n",
    "+-----------+             |Alerts &     |          |Disease Risk |\n",
    "|Fertilizer |             |Notifications|          +-------------+\n",
    "|Dispenser  |             +-------------+          |Water Stress |\n",
    "+-----------+                                      +-------------+\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
